% !TeX root = ../proceedings.tex
\section{Implementation}

\subsection{Language and tools}
The LegoLens application is built using Unity, where custom scripts are written in C\#. Unity is chosen, since it is used in Microsofts own Hololens academy.\footnote{\url{https://developer.microsoft.com/en-us/windows/mixed-reality/academy}} Microsoft also provides a software toolkit called HoloToolkit\footnote{\url{https://github.com/Microsoft/HoloToolkit-Unity}}, which binds Unity and the Hololens together. \\
For initial testing the Microsoft Hololens Emulator was used, while in later stages the application was deployed directly to the Hololens device. 

\subsection{HoloToolkit}
The HoloToolkit provide implementations of common tasks in developing applications for the Hololens. This kit implements functionality, which is mostly Hololens specific, such as spartial mapping and understanding. Also functionality for input methods are provided. The Hololens comes with two build-in gestures: the bloom and the tap.\\

\subsubsection{Sparital mapping and understanding}
These two functionalities are fundamental for the use of the Hololens, since they let the Hololens understand the shape of a room. Thereby virtual objects can be placed in the real world. The toolkit provides prefabricated functionality for spartial mapping and understanding, which simply can be added in Unity.  \\
The Hololens uses a depth camera similar to the camera build into the Kinect v2 and four cameras meant to understand environment.\footnote{\url{https://developer.microsoft.com/en-us/windows/mixed-reality/hololens_hardware_details}} The spatial mapping relies on the time-of-flight Kinect-like depth camera and the four RGB cameras to provide a robust tracking of the enviroment. 

\subsubsection{Input methods}
In this application only the tap method is used. The HoloToolkit provides functionality, such as dragging objects by holding the tap. 

The toolkit contains seven feature areas, of those spatial mapping and input were of most interest to us. We used the spatial mapping part of the toolkit to be able to "digitize" the world and make our application able to track surfaces so as to place our generator board on real world surfacs instead of having it float in mid air. This connection between the real-world and digital playground created in our application was essential, both to the experience but also to be able to call our application alternate reality.\\
The spatial mapping relies on the time-of-flight depth cameras and RGB cameras to provide a robust tracking of the enviroment. This mapping is then readily available to developers through the Holotoolkit and can be applied to object in an application.
(SKRIV MERE TEKNISK NÅR VI VED HVAD FUCK DER FOREGÅR) 
\\\\
The input part of the toolkit allows us to track the gaze and the users interaction with the objects in the application, be it buttons, bricks or the generator board. This tracking is done by shooting a ray from the users gaze (the middle of the screen on the Hololens in our case) and checking whether any colliders, object hitboxes, were hit. This raycasting, as its called, is intuitive since it uses the line of sight from the user to any object in the gameworld, so whatever you can see, you can interact with.\\
More specifically, the orientation and position of the Hololens with regards to the objects in the gameworld is maintained by a GazeManager from the HoloToolkit and the cursor is then placed on the vector originating from the users gaze by a CursorManager. This raycast depends on the Hololens ability to track the user using gyroscopes, accelerometers and computer vision. By tracking the user gaze in the real world and imposing a mapping from the real-world to a virtual-world coordinate system, the user can be mapped in with reference to the virtual objects (QUOTE EN TEKST, FÅ DET TIL AT LYDE MINDRE OSTET). 

\subsection{Unity scripts}
\subsubsection{Bricks}
\subsubsection{Generator}